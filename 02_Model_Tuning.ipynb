{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9fb1ea",
   "metadata": {},
   "source": [
    "# Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/Documents/GitHub/OGC-2025---Canopy-monitoring-hackathon/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4688e8",
   "metadata": {},
   "source": [
    "#### Loading training data\n",
    "\n",
    "The training dataset is loaded using [pandas](https://pandas.pydata.org/). After separating the predictors (X) from the variable to be estimated (y) the dataset is split into a dataset for training and testing the point estimator as well as a calibration dataset to conformalise the qunatile regressors used to estimate predictive uncertainty. While the training/testing split is done with a 80/20 ratio the testing/calibration split is conducted using a 50/50 ratio. The splitting is facilitated using the train_test_split functionality of [https://scikit-learn.org](scikit-learn). The coordinates were also detached from the data since they shall not be used as predictors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9886a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "training_data = pd.read_csv(\"./data/train.csv\")\n",
    "variables = list(training_data.columns)\n",
    "y = training_data[\"rh98\"]\n",
    "X = training_data.drop(columns=[\"rh98\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3513be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_calib, y_test, y_calib = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845c12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coords = X_train[[\"lat\", \"lon\"]]\n",
    "X_train.drop(columns=[\"lat\", \"lon\"], inplace=True)\n",
    "\n",
    "test_coords = X_test[[\"lat\", \"lon\"]]\n",
    "X_test.drop(columns=[\"lat\", \"lon\"], inplace=True)\n",
    "\n",
    "calib_coords = X_calib[[\"lat\", \"lon\"]]\n",
    "X_calib.drop(columns=[\"lat\", \"lon\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65275e56",
   "metadata": {},
   "source": [
    "#### Model tuning and fitting\n",
    "Before fiting the model, in this case a tree-base ensemble approach namely [Light Gradient Boosting](https://lightgbm.readthedocs.io/en/stable/), its hyperparameters are tuned on the training dataset using thr optimisation framework [Optuna](https://optuna.readthedocs.io/en/stable/). This is done by defining an objective function alongside the parameter space. The Roost Mean Squared Error is used as the score function which has to be minimize. The tuhing ran for 300 epochs. Optuna also provides a dashboard to reviev tuning performance and assess the importance of individual hyperparameters. In can be started using the following command: optuna-dashboard sqlite:///tuning.sqlite3. I can be accessed through: [http://127.0.0.1:8080/dashboard/](http://127.0.0.1:8080/dashboard/). The tuning results are stored in a sqlite database for later use or extension. The RSME was used as the loss function which had be minimized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6251300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 10, 1000)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.5)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 50)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "        reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0001, 100, log=True)\n",
    "        reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0001, 100, log=True)\n",
    "\n",
    "        model = LGBMRegressor(objective='regression',\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    max_depth=max_depth,\n",
    "                                    colsample_bytree=colsample_bytree,\n",
    "                                    subsample=subsample,\n",
    "                                    boosting_type='gbdt',\n",
    "                                    reg_alpha=reg_alpha,\n",
    "                                    reg_lambda=reg_lambda,\n",
    "                                    random_state=42,\n",
    "                                    n_jobs=14,\n",
    "                                    verbosity=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = root_mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(study_name=f\"OGH_2025_Canopy_Prediction_LightGradientBoosting\", direction=\"minimize\", storage=\"sqlite:///tuning.sqlite3\", load_if_exists=False)\n",
    "    study.optimize(objective, n_trials=300)\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "except:\n",
    "    print(\"Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe2c07",
   "metadata": {},
   "source": [
    "#### Fitting the LGBM regressor using the tuned hyperparameters\n",
    "The tuned regressor is fitted to the training dataset using the best performing hyperparameters from the study stored in the sqlite database. The trained model is subsequently pickled for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087606ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6750\n",
      "[LightGBM] [Info] Number of data points in the train set: 31628, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1681.126281\n"
     ]
    }
   ],
   "source": [
    "study_lgb = optuna.load_study(study_name=f\"OGH_2025_Canopy_Prediction_LightGradientBoosting\", storage=\"sqlite:///tuning.sqlite3\")\n",
    "lgb = LGBMRegressor(**study_lgb.best_params, random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "with open(f'./lightGradientBoosting.pickle', 'wb') as handle:\n",
    "        pickle.dump(lgb, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5432e",
   "metadata": {},
   "source": [
    "#### Tuning upper and lower quantile \n",
    "Point estimates alongside the performance indicators of the underlying estimator might be lacking utility and interpretability in certain use cases. Infromaton regading the uncertainly and reliability of the point estimates might improve on both aspects. Since the predictive uncertainty shall also be quantified two qunatile regressors are fitted to the data corresponding to the 0.025 and 0.975 quantile respectively. This was also done using optuna. The qunatile regressors are also fitted using the tuned hyperparameters and subsequently pickled for later use. While quantile regression is a straighforward way to quantify uncertainties it sure has its pitfalls as shown later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfac8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "miscoverage = 1-confidence\n",
    "alpha_low = round(miscoverage/2, 3)\n",
    "alpha_upp = round(1-miscoverage/2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b86b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\n",
      "Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial, alpha):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 1000)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.5)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 50)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0001, 100, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0001, 100, log=True)\n",
    "    \n",
    "    model = LGBMRegressor(objective='quantile',\n",
    "                          alpha=alpha,\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        subsample=subsample,\n",
    "                        boosting_type='gbdt',\n",
    "                        reg_alpha=reg_alpha,\n",
    "                        reg_lambda=reg_lambda,\n",
    "                        verbose=-1,\n",
    "                        random_state=42,\n",
    "                        n_jobs=12)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_train) \n",
    "    score = root_mean_squared_error(y_train, y_pred)\n",
    "    \n",
    "    return score\n",
    "\n",
    "try:\n",
    "  study_low = optuna.create_study(study_name=f\"OGH_2025_Canopy_Prediction_{alpha_low}_QuantileRegressor\", direction=\"minimize\", storage=\"sqlite:///tuning.sqlite3\", load_if_exists=False)\n",
    "  study_low.optimize(lambda trial: objective(trial, alpha=alpha_low), n_trials=300)\n",
    "except:\n",
    "    print(\"Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\")\n",
    "\n",
    "try:\n",
    "  study_upp = optuna.create_study(study_name=f\"OGH_2025_Canopy_Prediction_{alpha_upp}_QuantileRegressor\", direction=\"minimize\", storage=\"sqlite:///tuning.sqlite3\", load_if_exists=False)\n",
    "  study_upp.optimize(lambda trial: objective(trial, alpha=alpha_upp), n_trials=300)\n",
    "except:\n",
    "    print(\"Model has already been tuned. You may review this process under: http://127.0.0.1:8080/dashboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ce6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6750\n",
      "[LightGBM] [Info] Number of data points in the train set: 31628, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1681.126281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6750\n",
      "[LightGBM] [Info] Number of data points in the train set: 31628, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1681.126281\n"
     ]
    }
   ],
   "source": [
    "study_low = optuna.load_study(study_name=f\"OGH_2025_Canopy_Prediction_{alpha_low}_QuantileRegressor\", storage=\"sqlite:///tuning.sqlite3\")\n",
    "study_low = LGBMRegressor(**study_low.best_params, random_state=42)\n",
    "study_low.fit(X_train, y_train)\n",
    "with open(f'./{alpha_low}_quantileRegressor.pickle', 'wb') as handle:\n",
    "        pickle.dump(study_low, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "study_upp = optuna.load_study(study_name=f\"OGH_2025_Canopy_Prediction_{alpha_upp}_QuantileRegressor\", storage=\"sqlite:///tuning.sqlite3\")\n",
    "study_upp = LGBMRegressor(**study_upp.best_params, random_state=42)\n",
    "study_upp.fit(X_train, y_train)\n",
    "with open(f'./{alpha_upp}_quantileRegressor.pickle', 'wb') as handle:\n",
    "        pickle.dump(study_upp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
